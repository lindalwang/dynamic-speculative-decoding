# Speculative Decoding Base Configuration

# Model Configuration
models:
  target:
    name: "meta-llama/Llama-3.2-3B-Instruct"
    quantization: "int8"  # Options: "int8", "int4", null
  drafter:
    name: "meta-llama/Llama-3.2-1B-Instruct"
    quantization: "int8"  # Options: "int8", "int4", null

# Generation Parameters
generation:
  gamma: 4                    # Draft tokens per step
  max_length: 35              # Maximum tokens to generate
  use_cache: false            # Enable KV-cache
  chat_mode: true             

# Dynamic Gamma Scheduling
dynamic_gamma:
  enabled: true               # Enable dynamic gamma adjustment
  schedule: "heuristic"       # "heuristic" (adaptive) or "constant" (static)
  min_gamma: 1                # Minimum gamma
  max_gamma: 10               # Maximum gamma

# Sampling Strategy
sampling:
  sampler: "greedy"           # Options: greedy, multinomial, topk, nucleus, topknucleus
  temperature: 1.0
  top_k: 50                   # For topk and topknucleus
  top_p: 0.9                  # For nucleus and topknucleus

# Inference Modes (which methods to run)
inference_modes:
  speculative: true           # Run speculative decoding
  target_autoregressive: true # Run baseline target AR decoding
  drafter_autoregressive: false # Run drafter AR decoding

# Debug Settings
debug:
  enabled: false              # Show detailed step-by-step output
  seed: 42                    # Random seed for reproducibility

# Hardware
device: "cuda"                # Options: "cuda", "cpu", "mps"
